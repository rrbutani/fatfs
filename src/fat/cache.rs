//! Home of the `SectorCache` type; that which all writes and reads to `Storage`
//! flow through.

use super::types::SectorIdx;
use crate::util::{BitMap, BitMapLen};

use storage_traits::Storage;
use generic_array::{ArrayLength, GenericArray};

use core::cell::{RefCell, RefMut, Ref};
use core::cmp::Ordering;
use core::marker::PhantomData;
use core::ops::{Index, IndexMut};
use core::fmt::{self, Debug};

/// Counter type with interior mutability that implements `Copy`
/// (unlike `Cell<u64>`).
///
/// Extremely illegal.
#[derive(Debug, Clone, Copy)]
#[repr(transparent)]
pub struct CopyCounter(u64);

impl CopyCounter {
    fn new(v: u64) -> Self { Self(v) }

    fn set(&self, v: u64) -> u64 {
        #[allow(mutable_transmutes)] // TODO: this is UB!!! Switch to a Cell and use clone for the slice manipulation!
        let c = unsafe { core::mem::transmute::<&CopyCounter, &mut u64>(self) };

        let old = *c;
        *c = v;
        old
    }

    fn get(&self) -> u64 { self.0 }
}

#[derive(Debug, Clone, Copy)]
pub enum CacheEntry {
    /// Present but unmodified; can be freely evicted.
    Resident { s: SectorIdx, arr_idx: usize, age: u64, last_accessed: CopyCounter },
    /// Present and contains modifications.
    Dirty { s: SectorIdx, arr_idx: usize, age: u64, last_accessed: CopyCounter },
    /// Does not contain a sector.
    Free,
}

impl CacheEntry {
    /*pub */fn new(sector: SectorIdx, idx: usize, counter: &mut u64) -> Self {
        let age = *counter;
        *counter = counter.wrapping_add(1);

        if *counter < age { log::warn!("Internal cache counter overflowed!"); }

        Self::Resident { s: sector, arr_idx: idx, age, last_accessed: CopyCounter::new(0) }
    }

    fn new_for_lookup(s: SectorIdx) -> Self {
        Self::Resident { s, arr_idx: 0, age: 0, last_accessed: CopyCounter::new(0) }
    }

    /// Errors if the `CacheEntry` is `Free`, otherwise succeeds.
    /*pub */fn mark_as_dirty(&mut self) -> Result<(), ()> {
        use CacheEntry::*;
        *self = match *self {
            Resident { s, arr_idx, age, last_accessed } |
            Dirty { s, arr_idx, age, last_accessed } =>
                Dirty { s, arr_idx, age, last_accessed },
            Free => return Err(()),
        };

        Ok(())
    }

    /// Errors if the `CacheEntry` is not `Dirty`.
    /*pub */fn mark_as_clean(&mut self) -> Result<(), ()> {
        use CacheEntry::*;
        *self = match *self {
            Dirty { s, arr_idx, age, last_accessed } =>
                Resident { s, arr_idx, age, last_accessed },

            Resident { .. } | Free => return Err(()),
        };

        Ok(())
    }

    /*pub */fn is_dirty(&self) -> bool {
        matches!(self, CacheEntry::Dirty { .. })
    }

    /// `None` if the `CacheEntry` is `Free`; succeeds otherwise.
    /*pub */fn get_sector_idx(&self) -> Option<SectorIdx> {
        use CacheEntry::*;
        match self {
            Resident { s, .. } | Dirty { s, .. } => Some(*s),
            Free => None,
        }
    }

    /// `None` if the `CacheEntry` is `Free`; succeeds otherwise.
    /*pub */fn get_arr_idx(&self) -> Option<usize> {
        use CacheEntry::*;
        match self {
            Resident { arr_idx, .. } | Dirty { arr_idx, .. } => Some(*arr_idx),
            Free => None,
        }
    }

    /// Returns the previous accessed time on success and errors when the
    /// `CacheEntry` is `Free`.
    /*pub */fn accessed(&self, counter: &mut u64) -> Result<u64, ()> {
        use CacheEntry::*;

        let new_last_accessed = *counter;
        *counter = counter.wrapping_add(1);

        if *counter < new_last_accessed { log::warn!("Internal cache counter overflowed!"); }

        match self {
            Resident { last_accessed, .. } | Dirty { last_accessed, .. } => {
                let last = last_accessed.get();
                last_accessed.set(new_last_accessed);
                Ok(last)
            },
            Free => return Err(())
        }
    }
}

impl PartialEq for CacheEntry {
    fn eq(&self, other: &Self) -> bool {
        use CacheEntry::*;
        match (self, other) {
            // Once we get or patterns..
            // (Resident { s: a, .. } | Dirty { s: a, .. },
            //    Resident { s: b, .. } | Dirty { s: b, .. }) => a.eq(b),
            // (Resident { .. } | Dirty { .. }, Free) |
            // (Free, Resident { .. } | Dirty { .. }) => false
            // (Free, Free) => true

            (Resident { s: a, .. }, Resident { s: b, ..}) |
            (Resident { s: a, .. }, Dirty { s: b, ..}) |
            (Dirty { s: a, .. }, Resident { s: b, ..}) |
            (Dirty { s: a, .. }, Dirty { s: b, ..}) => a.eq(b),

            (Free, Free) => true,

            (Resident{ .. }, Free) |
            (Dirty { .. }, Free) |
            (Free, Resident { .. }) |
            (Free, Dirty { .. }) => false,
        }
    }
}

impl Eq for CacheEntry { }

impl PartialOrd for CacheEntry {
    #[must_use]
    #[inline]
    fn partial_cmp(&self, other: &CacheEntry) -> Option<Ordering> {
        use CacheEntry::*;
        Some(match (self, other) {
            // Again, or patterns would be nice here..

            (Resident { s: a, .. }, Resident { s: b, .. }) |
            (Resident { s: a, .. }, Dirty { s: b, .. }) |
            (Dirty { s: a, .. }, Resident { s: b, .. }) |
            (Dirty { s: a, .. }, Dirty { s: b, .. }) => a.cmp(b),

            (Free, Free) => Ordering::Equal,

            (Resident { .. }, Free) |
            (Dirty { .. }, Free) => Ordering::Less,

            (Free, Resident { .. }) |
            (Free, Dirty { .. }) => Ordering::Greater,
        })
    }
}

impl Ord for CacheEntry {
    #[must_use]
    #[inline]
    fn cmp(&self, other: &CacheEntry) -> Ordering {
        // The `PartialOrd` impl always returns `Some` so this is okay.
        self.partial_cmp(other).unwrap()
    }
}

impl Default for CacheEntry { fn default() -> Self { CacheEntry::Free } }

#[derive(Debug, Clone, PartialEq, Eq, Default)]
#[allow(non_camel_case_types)]
pub struct CacheTable<SIZE: ArrayLength<CacheEntry>> {
    // To help make cache lookups faster, we keep this in sorted order.
    cache_entry_table: GenericArray<CacheEntry, SIZE>,

    length: usize,
}

impl<S: ArrayLength<CacheEntry>> CacheTable<S> {
    pub fn new() -> Self {
        Default::default()
    }

    pub fn capacity() -> usize {
        S::to_usize()
    }

    pub fn len(&self) -> usize {
        self.length
    }

    pub fn free_entries(&self) -> usize {
        Self::capacity() - self.len()
    }

    /*pub */fn get(&self, s: SectorIdx) -> Option<&CacheEntry> {
        let entry = CacheEntry::new_for_lookup(s);
        self.cache_entry_table
            .as_slice()
            .binary_search(&entry)
            .ok()
            .map(|idx| &self.cache_entry_table.as_slice()[idx])
    }

    /*pub */fn get_mut(&mut self, s: SectorIdx) -> Option<&mut CacheEntry> {
        // Basically the same as the above save for the as_mut_slice calls.
        // Blame the borrow checker for the asymmetry.

        let entry = CacheEntry::new_for_lookup(s);
        match self.cache_entry_table
            .as_mut_slice()
            .binary_search(&entry)
            .ok() {
            Some(idx) => Some(&mut self.cache_entry_table[idx]),
            None => None,
        }
    }

    /// All newly inserted entries are marked as resident.
    ///
    /// Returns an `Err(Some(_))` if the table already contains an entry with
    /// the sector in question.
    ///
    /// Returns an `Err(None)` if we're out of space!
    /*pub */fn insert(
        &mut self,
        s: SectorIdx,
        idx: usize,
        counter: &mut u64,
    ) -> Result<&mut CacheEntry, Option<&mut CacheEntry>> {
        let entry = CacheEntry::new(s, idx, counter);
        match self.cache_entry_table.binary_search(&entry) {
            // If the sector is already in the table, return it's entry:
            Ok(idx) => {
                Err(Some(&mut self.cache_entry_table.as_mut_slice()[idx]))
            },

            Err(idx) => {
                // If it's not present, we were just told where to place this
                // entry.

                // First let's make sure we have room for it:
                if self.free_entries() == 0 {
                    return Err(None);
                }

                // Just to be extra sure, double check that the last element
                // really is free (since we're only adding one thing we only
                // need to check the last element):
                match self.cache_entry_table.as_slice().last() {
                    Some(last) => {
                        assert!(last == &CacheEntry::Free)
                    },
                    None => {
                        // Zero does satisfy the `Unsigned` trait so it's
                        // possible to construct an instance of this type with
                        // SIZE = 0, but the above check (free_entries >= 1)
                        // should catch this.
                        unreachable!()
                    },
                }

                // Now, shift everything at and after the index we were told to
                // insert into one place to the right. Note that we stop at
                // self.length() because there's no reason we need to bother
                // copying empty elements.
                self.cache_entry_table.copy_within(idx..(self.length), idx + 1);

                // Increment our length:
                self.length += 1;

                // And finally, put our new element into place and return it.
                let slot = &mut self.cache_entry_table[idx];
                *slot = entry;
                Ok(slot)
            }
        }
    }

    /// Tries to remove an entry with the given sector.
    ///
    /// Returns `Ok(arr_idx)` if the entry was successfully removed.
    ///
    /// Returns `Err(Some(_))` if the entry exists but is marked as dirty.
    ///
    /// Returns `Err(None)` if an entry for the sector does not exist.
    /*pub */fn remove(
        &mut self,
        s: SectorIdx
    ) -> Result<usize, Option<&mut CacheEntry>> {
        use CacheEntry::*;

        let entry = CacheEntry::new_for_lookup(s);
        match self.cache_entry_table.binary_search(&entry) {
            Ok(idx) => {
                match self.cache_entry_table[idx] {
                    Resident { arr_idx, .. } => {
                        // Move the remaining entries left one.
                        //
                        // | a | b | c | E | e | f | _ | _ | _ | _ |
                        //                  \     /
                        //                  copy to:
                        //                     |
                        //                 /---/
                        //                 V
                        //              /     \
                        // | a | b | c | E | e | f | _ | _ | _ | _ |
                        // | a | b | c | e | f | f | _ | _ | _ | _ |
                        //
                        // And then zero the last element:
                        // | a | b | c | e | f | f | _ | _ | _ | _ |
                        //
                        //                   |
                        //                   V
                        //
                        // | a | b | c | e | f | _ | _ | _ | _ | _ |
                        //
                        // This works even when there are no following entries.

                        self.cache_entry_table
                            .copy_within((idx + 1)..(self.length), idx);

                        self.length -= 1;
                        self.cache_entry_table[self.length] = CacheEntry::Free;

                        Ok(arr_idx)
                    },

                    // If it's dirty, error:
                    Dirty { .. } => Err(Some(&mut self.cache_entry_table[idx])),

                    // This can't happen; lookup _can't_ return a Free sector.
                    Free => unreachable!(),
                }
            },

            Err(_) => {
                // If a corresponding Entry is not present, error:
                Err(None)
            }
        }
    }

    /// Calls a function on every dirty `CacheEntry`.
    /*pub */fn for_each_dirty_entry<E, F: FnMut((usize, &mut CacheEntry)) -> Result<(), E>>(
        &mut self,
        func: F,
    ) -> Result<(), E> {
        self.cache_entry_table.iter_mut()
            .enumerate()
            .filter(|(_, e)| e.is_dirty())
            .map(func)
            .collect()
    }
}

pub trait EvictionPolicy {
    /// Pick which cache entry, between the two given, you'd rather evict.
    ///
    /// Implementors are encouraged to use an ordering that is _asymmetric_,
    /// _transitive_, and _total_.
    ///
    /// This is basically [`Ord`].
    ///
    /// This only takes &self to be object safe.
    fn compare(&self, a: &CacheEntry, b: &CacheEntry) -> Ordering;

    /// Returns `None` if there are no elements in the array.
    fn pick_entry_to_evict<'arr>(&self, arr: &'arr mut [CacheEntry]) -> Option<&'arr mut CacheEntry> {
        arr.iter_mut()
            .max_by(|a, b| self.compare(a, b))
    }
}

pub type DynEvictionPolicy = &'static (dyn EvictionPolicy + Send + Sync + 'static);

impl EvictionPolicy for DynEvictionPolicy {
    #[inline]
    fn compare(&self, a: &CacheEntry, b: &CacheEntry) -> Ordering {
        (*self).compare(a, b)
    }
}

pub mod eviction_policies {
    use super::{CacheEntry::{self, *}, Ordering, EvictionPolicy, DynEvictionPolicy};

    macro_rules! policy {
        ($name:ident ($instance:ident): $($arms:tt)*) => {
            #[derive(Debug, Copy, Clone, PartialEq, Eq, PartialOrd, Ord,
                Hash, Default
            )]
            pub struct $name;
            pub static $instance: DynEvictionPolicy = &$name;

            impl EvictionPolicy for $name {
                fn compare(&self, a: &CacheEntry, b: &CacheEntry) -> Ordering {
                    match (a, b) {
                        $($arms)*

                        (Free, Resident { .. }) |
                        (Free, Dirty { .. }) => Ordering::Greater,

                        (Resident { .. }, Free) |
                        (Dirty { .. }, Free) => Ordering::Less,

                        (Free, Free) => Ordering::Equal,
                    }
                }
            }
        };

        (<$inner:ident as $instance:ident> $name:ident with ($a:ident, $b:ident): $($arms:tt)*) => {
            #[derive(Debug, Copy, Clone, PartialEq, Eq, PartialOrd, Ord,
                Hash, Default
            )]
            pub struct $name<$inner: EvictionPolicy>($inner);

            impl<$inner: EvictionPolicy> EvictionPolicy for $name<$inner> {
                fn compare(&self, $a: &CacheEntry, $b: &CacheEntry) -> Ordering {
                    let $instance: &$inner = &self.0;

                    match ($a, $b) {
                        $($arms)*

                        (Free, Resident { .. }) |
                        (Free, Dirty { .. }) => Ordering::Greater,

                        (Resident { .. }, Free) |
                        (Dirty { .. }, Free) => Ordering::Less,

                        (Free, Free) => Ordering::Equal,
                    }
                }
            }
        };
    }

    policy! { Youngest (YOUNGEST):
        (Resident { age: a, .. }, Resident { age: b, .. }) |
        (Resident { age: a, .. }, Dirty { age: b, .. }) |
        (Dirty { age: a, .. }, Resident { age: b, .. }) |
        (Dirty { age: a, .. }, Dirty { age: b, .. }) => a.cmp(b),
    }

    // Unfortunately, because of the (Free, _) and (_, Free) cases this can't
    // just be `Youngest::cmp(_, _).reverse()`.
    policy! { Oldest (OLDEST):
        (Resident { age: a, .. }, Resident { age: b, .. }) |
        (Resident { age: a, .. }, Dirty { age: b, .. }) |
        (Dirty { age: a, .. }, Resident { age: b, .. }) |
        (Dirty { age: a, .. }, Dirty { age: b, .. }) => a.cmp(b).reverse(),
    }

    policy! { MostRecentlyAccessed (MOST_RECENTLY_ACCESSED):
        (Resident { last_accessed: a, .. }, Resident { last_accessed: b, .. }) |
        (Resident { last_accessed: a, .. }, Dirty { last_accessed: b, .. }) |
        (Dirty { last_accessed: a, .. }, Resident { last_accessed: b, .. }) |
        (Dirty { last_accessed: a, .. }, Dirty { last_accessed: b, .. })
            => a.get().cmp(&b.get()),
    }

    policy! { LeastRecentlyAccessed (LEAST_RECENTLY_ACCESSED):
        (Resident { last_accessed: a, .. }, Resident { last_accessed: b, .. }) |
        (Resident { last_accessed: a, .. }, Dirty { last_accessed: b, .. }) |
        (Dirty { last_accessed: a, .. }, Resident { last_accessed: b, .. }) |
        (Dirty { last_accessed: a, .. }, Dirty { last_accessed: b, .. })
            => a.get().cmp(&b.get()).reverse(),
    }

    policy! { <Inner as inner> UnmodifiedFirst with (a, b):
        (Resident { .. }, Dirty { .. }) => Ordering::Greater,
        (Dirty { .. }, Resident { .. }) => Ordering::Less,

        (Resident { .. }, Resident { .. }) |
        (Dirty { .. }, Dirty { .. }) => inner.compare(a, b),
    }

    /// Prefers unmodified entries over modified entries, with entry age being
    /// used as a tiebreaker (younger entries are picked over older ones).
    pub static UNMODIFIED_THEN_YOUNGEST: DynEvictionPolicy =
        &UnmodifiedFirst::<Youngest>(Youngest);

    /// Prefers unmodified entries over modified entries, with entry age being
    /// used as a tiebreaker (older entries are picked over younger ones).
    pub static UNMODIFIED_THEN_OLDEST: DynEvictionPolicy =
        &UnmodifiedFirst::<Oldest>(Oldest);

    /// Prefers unmodified entries over modified entries, with last entry access
    /// being used as a tiebreaker (more recently accessed entries are
    /// preferred).
    pub static UNMODIFIED_THEN_MOST_RECENTLY_ACCESSED: DynEvictionPolicy =
        &UnmodifiedFirst::<MostRecentlyAccessed>(MostRecentlyAccessed);

    /// Prefers unmodified entries over modified entries, with last entry access
    /// being used as a tiebreaker (less recently accessed entries are
    /// preferred).
    pub static UNMODIFIED_THEN_LEAST_RECENTLY_ACCESSED: DynEvictionPolicy =
        &UnmodifiedFirst::<LeastRecentlyAccessed>(LeastRecentlyAccessed);

    policy! { <Inner as inner> ModifiedFirst with (a, b):
        (Dirty { .. }, Resident { .. }) => Ordering::Greater,
        (Resident { .. }, Dirty { .. }) => Ordering::Less,

        (Resident { .. }, Resident { .. }) |
        (Dirty { .. }, Dirty { .. }) => inner.compare(a, b),
    }

    /// Prefers modified entries over unmodified entries, with entry age being
    /// used as a tiebreaker (younger entries are picked over older ones).
    pub static MODIFIED_THEN_YOUNGEST: DynEvictionPolicy =
        &ModifiedFirst::<Youngest>(Youngest);

    /// Prefers modified entries over unmodified entries, with entry age being
    /// used as a tiebreaker (older entries are picked over younger ones).
    pub static MODIFIED_THEN_OLDEST: DynEvictionPolicy =
        &ModifiedFirst::<Oldest>(Oldest);

    /// Prefers modified entries over unmodified entries, with last entry access
    /// being used as a tiebreaker (more recently accessed entries are
    /// preferred).
    pub static MODIFIED_THEN_MOST_RECENTLY_ACCESSED: DynEvictionPolicy =
        &ModifiedFirst::<MostRecentlyAccessed>(MostRecentlyAccessed);

    /// Prefers modified entries over unmodified entries, with last entry access
    /// being used as a tiebreaker (less recently accessed entries are
    /// preferred).
    pub static MODIFIED_THEN_LEAST_RECENTLY_ACCESSED: DynEvictionPolicy =
        &ModifiedFirst::<LeastRecentlyAccessed>(LeastRecentlyAccessed);

}

#[allow(non_camel_case_types)]
pub struct SectorCache<StorageImpl, SECTOR_SIZE, CACHE_SIZE_IN_SECTORS, Eviction = DynEvictionPolicy>
where
    StorageImpl: Storage<Word = u8, SECTOR_SIZE = SECTOR_SIZE>,
    SECTOR_SIZE: ArrayLength<u8>,
    CACHE_SIZE_IN_SECTORS: ArrayLength<RefCell<GenericArray<u8, SECTOR_SIZE>>>,
    CACHE_SIZE_IN_SECTORS: ArrayLength<CacheEntry>,
    CACHE_SIZE_IN_SECTORS: BitMapLen,
    Eviction: EvictionPolicy,
{
    cached_sectors: GenericArray<RefCell<GenericArray<u8, SECTOR_SIZE>>, CACHE_SIZE_IN_SECTORS>,
    cache_table: CacheTable<CACHE_SIZE_IN_SECTORS>,
    cache_bitmap: BitMap<CACHE_SIZE_IN_SECTORS>,

    num_sectors: SectorIdx,

    eviction_policy: Eviction,
    counter: RefCell<u64>,

    _s: PhantomData<StorageImpl>,
}

#[allow(non_camel_case_types)]
impl<S, SECT_SIZE, CACHE_SIZE, Ev> Debug for SectorCache<S, SECT_SIZE, CACHE_SIZE, Ev>
where
    S: Storage<Word = u8, SECTOR_SIZE = SECT_SIZE>,
    SECT_SIZE: ArrayLength<u8>,
    CACHE_SIZE: ArrayLength<RefCell<GenericArray<u8, SECT_SIZE>>>,
    CACHE_SIZE: ArrayLength<CacheEntry>,
    CACHE_SIZE: BitMapLen,
    Ev: EvictionPolicy,
{
    fn fmt(&self, fmt: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(fmt, "TODO!")
    }
}

#[allow(non_camel_case_types)]
impl<S, SECT_SIZE, CACHE_SIZE, Ev> SectorCache<S, SECT_SIZE, CACHE_SIZE, Ev>
where
    S: Storage<Word = u8, SECTOR_SIZE = SECT_SIZE>,
    SECT_SIZE: ArrayLength<u8>,
    CACHE_SIZE: ArrayLength<RefCell<GenericArray<u8, SECT_SIZE>>>,
    CACHE_SIZE: ArrayLength<CacheEntry>,
    CACHE_SIZE: BitMapLen,
    Ev: EvictionPolicy,
{
    pub /*const*/ fn cache_size_in_bytes() -> usize {
        SECT_SIZE::to_usize() * CACHE_SIZE::to_usize()
    }

    pub fn new(_witness: &S, num_sectors: SectorIdx, ev: Ev) -> Self {
        Self {
            cached_sectors: Default::default(),
            cache_table: CacheTable::new(),
            cache_bitmap: BitMap::new(),

            num_sectors,

            eviction_policy: ev,
            counter: RefCell::new(0),

            _s: PhantomData,
        }
    }

    /// Returns `Err` if there are no entries there to evict.
    /*pub */fn evict_entry(&mut self, storage: &mut S) -> Result<(), ()> {
        if self.cache_table.len() == 0 { return Err(()); }

        let entry = self.eviction_policy.pick_entry_to_evict(
                &mut self.cache_table.cache_entry_table)
            .expect("must give an entry to evict when the cache table is not \
                empty");

        let sector_idx = entry.get_sector_idx().expect("dirty entries have a sector index");
        let arr_idx = entry.get_arr_idx().expect("dirty entries have an arr index");

        // Check if the entry we're to remove is dirty:
        if entry.is_dirty() {
            // If it is, write it out:
            storage.write_sector(
                sector_idx.idx(),
                // We do a mutable borrow here even though we don't _need_ to
                // because we want to make sure that no one else has a reference
                // to this sector that's being evicted. While we don't remove
                // the sector or overwrite it here (which is why we don't need
                // a mutable reference) we're presumably about to.
                &self.cached_sectors[arr_idx]
                    .try_borrow_mut()
                    .expect("no references to a sector we're about to evict"),
            ).unwrap();

            // And mark it as clean:
            entry.mark_as_clean().unwrap();
        }

        // And finally, remove it from the table and the bitmap:
        self.cache_table.remove(sector_idx).expect("to be able to remove clean entries");
        self.cache_bitmap.set(arr_idx, false).unwrap();

        Ok(())
    }

    // Since storage has to be passed into us, unfortunately we can't do this
    // on Drop...
    pub fn flush(&mut self, storage: &mut S) -> Result<(), ()> {
        let ref cached_sectors = self.cached_sectors;

        self.cache_table.for_each_dirty_entry(|(idx, e)| {
            storage.write_sector(
                e.get_sector_idx().expect("dirty entries have a sector index").idx(),
                // We don't actually need a mutable borrow here but, as the
                // message below explains, we should always get it and it's a
                // good sanity test.
                &cached_sectors[idx]
                    .try_borrow_mut()
                    .expect("no references to any sectors when we have a mutable reference to the sector cache"),
            ).unwrap();

            e.mark_as_clean()
        })
    }

    pub fn upgrade<'s>(
        &'s mut self,
        storage: &'s mut S
    ) -> SectorCacheWithStorage<'s, S, SECT_SIZE, CACHE_SIZE, Ev, UnIndexable> {
        // TODO: should we enable flush on Drop here?

        SectorCacheWithStorage::new(self, storage)
    }

    pub fn get_sector_entry(
        &mut self,
        storage: &mut S,
        index: SectorIdx,
    ) -> (&mut CacheEntry, RefMut<'_, u64>) {
        // See if we've already got this sector in the cache:
        if let Some(_) = self.cache_table.get(index) {
            // return c; // Unfortunately the borrow checker is not smart enough
                         // to see that this arm is mutually exclusive from the
                         // other arm because of the return.
        } else {
            // If we don't, try to load it into the cache.

            // First, let's get the index where we can place the sector:
            let idx = match self.cache_bitmap.next_empty_bit() {
                Ok(idx) => idx,
                Err(()) => {
                    // If the cache is full, we need to evict a sector.
                    self.evict_entry(storage).expect("eviction to succeed");

                    // Now, we can try to get an index again; this time it
                    // _must_ succeed:
                    self.cache_bitmap.next_empty_bit().expect("an empty sector after eviction")
                },
            };

            // Load the sector in:
            // (it's a little silly that we go lookup the index to this sector
            // again but it's worth it for maintaining the symmetry)
            storage.read_sector(
                index.idx(),
                &mut self.cached_sectors[idx].try_borrow_mut().expect("clean entries to have no references")
            ).unwrap();

            // Add to the cache table and the bitmap:
            self.cache_bitmap.set(idx, true).unwrap();
            match self.cache_table
                    .insert(index, idx, &mut self.counter.borrow_mut()) {
                Ok(_entry) => /*entry*/ {},

                // It's not possible that we're out of space; the cache bitmap
                // gave us an index.
                Err(None) => unreachable!(),

                // It's not possible that this sector is already cached; we
                // started by looking it up.
                Err(Some(_)) => unreachable!(),
            }
        }

        (self.cache_table.get_mut(index).unwrap(), self.counter.borrow_mut())
    }
}

#[allow(non_camel_case_types)]
impl<S, SECT_SIZE, CACHE_SIZE> SectorCache<S, SECT_SIZE, CACHE_SIZE, DynEvictionPolicy>
where
    S: Storage<Word = u8, SECTOR_SIZE = SECT_SIZE>,
    SECT_SIZE: ArrayLength<u8>,
    CACHE_SIZE: ArrayLength<RefCell<GenericArray<u8, SECT_SIZE>>>,
    CACHE_SIZE: ArrayLength<CacheEntry>,
    CACHE_SIZE: BitMapLen,
{
    pub fn change_eviction_policy(&mut self, ev: DynEvictionPolicy) {
        self.eviction_policy = ev
    }
}

#[allow(non_camel_case_types)]
impl<S, SECT_SIZE, CACHE_SIZE, Ev> Drop for SectorCache<S, SECT_SIZE, CACHE_SIZE, Ev>
where
    S: Storage<Word = u8, SECTOR_SIZE = SECT_SIZE>,
    SECT_SIZE: ArrayLength<u8>,
    CACHE_SIZE: ArrayLength<RefCell<GenericArray<u8, SECT_SIZE>>>,
    CACHE_SIZE: ArrayLength<CacheEntry>,
    CACHE_SIZE: BitMapLen,
    Ev: EvictionPolicy,
{
    fn drop(&mut self) {
        let mut i = 0;

        self.cache_table.for_each_dirty_entry::<(), _>(|_| {
            i += 1;

            Ok(())
        }).unwrap();

        if i != 0 {
            panic!("A SectorCache was dropped with dirty entries ({} of them)!", i);
        }
    }
}

pub struct UnIndexable;
pub struct Indexable;

#[allow(non_camel_case_types)]
#[derive(Debug)]
pub struct SectorCacheWithStorage<'s, StorageImpl, SECTOR_SIZE, CACHE_SIZE_IN_SECTORS, Eviction, Ty = UnIndexable>
where
    StorageImpl: Storage<Word = u8, SECTOR_SIZE = SECTOR_SIZE>,
    SECTOR_SIZE: ArrayLength<u8>,
    CACHE_SIZE_IN_SECTORS: ArrayLength<RefCell<GenericArray<u8, SECTOR_SIZE>>>,
    CACHE_SIZE_IN_SECTORS: ArrayLength<CacheEntry>,
    CACHE_SIZE_IN_SECTORS: BitMapLen,
    Eviction: EvictionPolicy,
{
    sector_cache: RefCell<&'s mut SectorCache<StorageImpl, SECTOR_SIZE, CACHE_SIZE_IN_SECTORS, Eviction>>,
    storage: RefCell<&'s mut StorageImpl>,

    flush_on_drop: bool,

    _ty: PhantomData<Ty>,
}

#[allow(non_camel_case_types)]
impl<'s, S, SS, CS, Ev, Ty> SectorCacheWithStorage<'s, S, SS, CS, Ev, Ty>
where
    S: Storage<Word = u8, SECTOR_SIZE = SS>,
    SS: ArrayLength<u8>,
    CS: ArrayLength<RefCell<GenericArray<u8, SS>>>,
    CS: ArrayLength<CacheEntry>,
    CS: BitMapLen,
    Ev: EvictionPolicy,
{
    pub fn flush_on_drop(&mut self, enable: bool) {
        self.flush_on_drop = enable
    }

    fn refs(&self) -> (RefMut<&'s mut SectorCache<S, SS, CS, Ev>>, RefMut<&'s mut S>) {
        (self.sector_cache.borrow_mut(), self.storage.borrow_mut())
    }

    /// Note: this will panic if, in order to load the requested sector, we end
    /// up needing to evict a sector that has a borrow currently out.
    pub fn get<'r>(&'r self, index: SectorIdx) -> Ref<'r, GenericArray<u8, SS>> {
        let arr_idx = self.get_inner(index, false);

        #[allow(unsafe_code)]
        // I think this is safe; we're getting an untracked reference to the
        // sector cache but only taking a tracked reference out of one of its
        // components. Accesses to the sector cache ref cell aren't aware of
        // this but any accesses to the actual sector will be, which is why I
        // think this is okay.
        //
        // There's possibly a better way to get a reference out of nested
        // RefCells. Or a better way to do this entire thing.
        let sector_cache_ref = unsafe {
            self.sector_cache.try_borrow_unguarded().unwrap()
        };

        sector_cache_ref.cached_sectors[arr_idx]
            .try_borrow()
            .expect("immutable sector borrows always succeed")
    }

    // Note: this will panic if, in order to load the requested sector, we end
    // up needing to evict a sector that has a borrow currently out.
    fn get_inner(&self, index: SectorIdx, mark_as_dirty: bool) -> usize {
        let (mut sector_cache, mut storage) = self.refs();

        assert!(index < sector_cache.num_sectors);

        let (cache_entry, mut counter) =
            sector_cache.get_sector_entry(&mut storage, index);

        // Mark the entry as accessed.
        cache_entry
            .accessed(&mut *counter)
            .expect("entry isn't `Free`");

        // Mark as dirty if necessary.
        // Note: this is imperfect; ideally we'd check if the user actually
        // wrote something different before marking. Not sure how we could
        // actually do something like that though. Checking if a sector is dirty
        // purely when evicting it doesn't seem like it'd work (i.e. checksums
        // and hashes can collide and grabbing an original copy to check against
        // would be more expensive than just doing the write so checksums/hashes
        // actually can't help us for this). So maybe we can't really do better
        // than this. (TODO)
        if mark_as_dirty {
            cache_entry.mark_as_dirty().unwrap();
        }

        // Finally, get the entry's corresponding sector cache array:
        cache_entry
            .get_arr_idx()
            .expect("entry has an arr index")
    }

    pub fn get_mut(&mut self, index: SectorIdx) -> &mut GenericArray<u8, SS> {
        let arr_idx = self.get_inner(index, true);

        // hey look! no unsafe
        self.sector_cache.get_mut().cached_sectors[arr_idx].get_mut()
    }
}

#[allow(non_camel_case_types)]
impl<'s, S, SS, CS, Ev> SectorCacheWithStorage<'s, S, SS, CS, Ev, UnIndexable>
where
    S: Storage<Word = u8, SECTOR_SIZE = SS>,
    SS: ArrayLength<u8>,
    CS: ArrayLength<RefCell<GenericArray<u8, SS>>>,
    CS: ArrayLength<CacheEntry>,
    CS: BitMapLen,
    Ev: EvictionPolicy,
{
    pub fn new(sc: &'s mut SectorCache<S, SS, CS, Ev>, stor: &'s mut S) -> Self {
        Self {
            sector_cache: RefCell::new(sc),
            storage: RefCell::new(stor),

            flush_on_drop: false,

            _ty: PhantomData,
        }
    }

    /// It turns out we cannot safely implement Index (and therefore IndexMut)
    /// for this type,
    ///
    /// The issue is that in Index we really mutate the underlying sector cache
    /// (when evicting sectors) but we also give out references to specific
    /// sectors _within_ the cache. This is a problem because it means it's
    /// possible for us to evict a sector that someone already has a reference
    /// to!
    ///
    /// If we want to support having multiple borrows of sectors out at once
    /// from this type (this isn't supported for mutable borrows of sectors
    /// which is why IndexMut doesn't have this problem) we need to turn to some
    /// kind of reference counting (I'm reasonably sure we can't reason about
    /// this at compile time since cache eviction is dynamic and all).
    ///
    /// We can have each borrow be given a type like `Ref` that maintains a
    /// reference count variable for the sector that is being borrowed. On Drop,
    /// it decrements the reference count (which means that it needs a reference
    /// to the instance of this type). If this sounds a lot like RefCell, that's
    /// because it is basically RefCell.
    ///
    /// Rather than reinvent the wheel we can just wrap our sectors in RefCells
    /// and give out leases to those for `get` and `get_mut`. This then becomes
    /// safe (and possible to write in safe Rust!).
    ///
    /// For Index, because we _must_ return references to things we cannot do
    /// this (i.e. we can't return a concrete type with a Drop impl and we
    /// can't impl Drop on a reference — even for a type we own). Even so, we
    /// offer an Index and IndexMut impl for a variant of this type. You have
    /// to use this function to make said variant of the type and this function
    /// is unsafe because you have to promise that you'll make sure you don't
    /// hold onto sectors that get evicted.
    pub unsafe fn make_indexable(self) -> SectorCacheWithStorage<'s, S, SS, CS, Ev, Indexable> {
        // let flush_on_drop = self.flush_on_drop;
        // self.flush_on_drop = false;

        // SectorCacheWithStorage {
        //     sector_cache: self.sector_cache.clone(),
        //     storage: self.storage.clone(),
        //     flush_on_drop,
        //     _ty: PhantomData
        // }

        // I think this is safe..
        #[allow(unused_unsafe)]
        unsafe { core::mem::transmute(self) }
    }
}

#[allow(non_camel_case_types)]
impl<'s, S, SS, CS, Ev, Ty> Drop for SectorCacheWithStorage<'s, S, SS, CS, Ev, Ty>
where
    S: Storage<Word = u8, SECTOR_SIZE = SS>,
    SS: ArrayLength<u8>,
    CS: ArrayLength<RefCell<GenericArray<u8, SS>>>,
    CS: ArrayLength<CacheEntry>,
    CS: BitMapLen,
    Ev: EvictionPolicy,
{
    fn drop(&mut self) {
        if self.flush_on_drop {
            self.sector_cache.borrow_mut().flush(&mut self.storage.borrow_mut()).unwrap()
        }
    }
}

#[allow(non_camel_case_types)]
impl<'s, S, SECT_SIZE, CACHE_SIZE, Ev> Index<SectorIdx> for SectorCacheWithStorage<'s, S, SECT_SIZE, CACHE_SIZE, Ev, Indexable>
where
    S: Storage<Word = u8, SECTOR_SIZE = SECT_SIZE>,
    SECT_SIZE: ArrayLength<u8>,
    CACHE_SIZE: ArrayLength<RefCell<GenericArray<u8, SECT_SIZE>>>,
    CACHE_SIZE: ArrayLength<CacheEntry>,
    CACHE_SIZE: BitMapLen,
    Ev: EvictionPolicy,
{
    type Output = GenericArray<u8, SECT_SIZE>;

    fn index(&self, index: SectorIdx) -> &GenericArray<u8, SECT_SIZE> {
        // Ideally we'd just call `get` here and use `Ref::leak` but that
        // requires nightly so we end up having to copy over the contents of
        // that function here (get_inner exists so we don't have to copy
        // _everything_).

        let arr_idx = self.get_inner(index, false);

        unsafe {
            self.sector_cache
                .try_borrow_unguarded() // I think this is safe (see `get`)
                .unwrap()
                .cached_sectors[arr_idx]
                .try_borrow_unguarded() // This is potentially dangerous but they opted in.
                .unwrap()
        }
    }
}

#[allow(non_camel_case_types)]
impl<'s, S, SECT_SIZE, CACHE_SIZE, Ev> IndexMut<SectorIdx> for SectorCacheWithStorage<'s, S, SECT_SIZE, CACHE_SIZE, Ev, Indexable>
where
    S: Storage<Word = u8, SECTOR_SIZE = SECT_SIZE>,
    SECT_SIZE: ArrayLength<u8>,
    CACHE_SIZE: ArrayLength<RefCell<GenericArray<u8, SECT_SIZE>>>,
    CACHE_SIZE: ArrayLength<CacheEntry>,
    CACHE_SIZE: BitMapLen,
    Ev: EvictionPolicy,
{
    fn index_mut(&mut self, index: SectorIdx) -> &mut GenericArray<u8, SECT_SIZE> {
        self.get_mut(index)
    }
}

// TODO: i wonder if it'd be practical to implement Index<Range<Sector>> for
// SectorCache.. the problems (that i can think of immediately) are:
//   - what happens when you ask for more sectors than we can accommodate
//   - what happens when some (or all) of the sectors you ask for are cached
//     but aren't back to back?
//
// Rearranging sectors to make sure we have a contiguous slice also sounds
// pretty painful.. we also don't actually have a contiguous block of memory
// underneath us (we've got a GenericArray of GenericArrays) so I think that
// makes this pretty much impossible without changing the internal to actually
// just use one huge u8 array (length = Mul<SECTOR_SIZE, CACHE_SIZE>).
//
// In any case, the use case for having an actually contiguous array of memory
// that represents a file seems extremely small/niche.
